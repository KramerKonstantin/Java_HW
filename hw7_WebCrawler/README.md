Потокобезопасный класс WebCrawler, который будет рекурсивно обходить сайты.
1. Класс WebCrawler имеет конструктор
	public WebCrawler(Downloader downloader, int downloaders, int extractors, int perHost)
2. downloader позволяет скачивать страницы и извлекать из них ссылки;
3. downloaders — максимальное число одновременно загружаемых страниц;
4. extractors — максимальное число страниц, из которых извлекаются ссылки;
5. perHost — максимальное число страниц, одновременно загружаемых c одного хоста. Для опредения хоста используется метод getHost класса URLUtils из тестов.
5. Класс WebCrawler реализует интерфейс Crawler
                        public interface Crawler extends AutoCloseable {
                            List<String> download(String url, int depth) throws IOException;

                            void close();
                        }
                    
6. Метод download рекурсивно обходит страницы, начиная с указанного URL на указанную глубину и возвращает список загруженных страниц и файлов. Например, если глубина равна 1, то загружена только указанная страница. Если глубина равна 2, то указанная страница и те страницы и файлы, на которые она ссылается и так далее. Этот метод может вызываться параллельно в нескольких потоках.
7. Загрузка и обработка страниц (извлечение ссылок) выполняется максимально параллельно, с учетом ограничений на число одновременно загружаемых страниц (в том числе с одного хоста) и страниц, с которых загружаются ссылки.
8. Для распараллеливания разрешается создать до downloaders + extractors вспомогательных потоков.
9. Загружать и/или извлекать ссылки из одной и той же страницы в рамках одного обхода (download) запрещается.
10. Метод close завершает все вспомогательные потоки.
11. Для загрузки страниц применяется Downloader, передаваемый первым аргументом конструктора.
public interface Downloader {
	public Document download(final String url) throws IOException;
}
                    
12. Метод download загружает документ по его адресу (URL).
13. Документ позволяет получить ссылки по загруженной странице: 
public interface Document {
	List<String> extractLinks() throws IOException;
}
                
14. Ссылки, возвращаемые документом являются абсолютными и имеют схему http или https.
15. Реализован метод main, позволяющий запустить обход из командной строки
Командная строка: WebCrawler url [depth [downloads [extractors [perHost]]]]
16. Для загрузки страниц требуется использовать реализацию CachingDownloader из тестов.
